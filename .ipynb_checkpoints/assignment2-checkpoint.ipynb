{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Deep Q learning\n",
    "\n",
    "The problem with Q-learning is that the size of the Q-table becomes infeasible for large boards. Let's take it one step further and train a Deep Q model, which can handle more.\n",
    "\n",
    "**Task**: Import necessary packages. Could take a little more time than with the previous assignment. Watch the asterisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "nbimporter.options['only_defs'] = False\n",
    "from pacman_game import Action, initialize_gamestate_from_file, get_next_game_state_from_action, ActionEvent\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.02\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "**Task:** As with Q-learning, specify the reward values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_for_move(action_event):\n",
    "    if action_event == ActionEvent.DOT:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.CAPTURED_BY_GHOST:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.NONE:\n",
    "        return -0.1\n",
    "    elif action_event == ActionEvent.WALL:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.WON:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.LOST:\n",
    "        # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "A possible neural net model is for the most part implemented below. The input is the board, and thus the input_size must correspond with the size of the board (number of board elements). For level 0, this is 60. If you change the board to train on, the input size must also change. The model will use Stochastic Gradient Descent for optimizing loss and Mean-Squared-Error as error function. The convert_state_to_input method converts board elements (pacman, ghost, dot, etc..) to one-hot vectors, and represents the board as one long list of one-hot vectors concatenated together.\n",
    "\n",
    "**Tasks:** \n",
    "1. Assign num_actions. Get the number of actions by getting len() of Action.get_all_actions(). This number corresponds to number of ouput neurons in the network. \n",
    "\n",
    "\n",
    "2. Complete the network by adding an output layer to the model after the last hidden layer. Use Dense() which takes number of neurons as argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_input(state):\n",
    "    string_rep = state.__str__()\n",
    "    r = np.array([])\n",
    "\n",
    "    for char in string_rep:\n",
    "        if char == 'o':\n",
    "            r = np.concatenate([r, [0, 0, 0, 0, 1]])\n",
    "        if char == ' ':\n",
    "            r = np.concatenate([r, [0, 0, 0, 1, 0]])\n",
    "        if char == 'P':\n",
    "            r = np.concatenate([r, [0, 0, 1, 0, 0]])\n",
    "        if char == 'G':\n",
    "            r = np.concatenate([r, [0, 1, 0, 0, 0]])\n",
    "        if char == '.':\n",
    "            r = np.concatenate([r, [1, 0, 0, 0, 0]])\n",
    "\n",
    "    return r.reshape(1, r.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE MODEL ##\n",
    "level = 'level-0'\n",
    "initial_game_state = initialize_gamestate_from_file(level)\n",
    "input_size = convert_state_to_input(initial_game_state).size\n",
    "\n",
    "# TODO: Get number of actions\n",
    "num_actions = \n",
    "\n",
    "model = Sequential()  # Input layer is embedded\n",
    "model.add(Dense(256, input_shape=(input_size,), activation='relu'))  # Hidden layer\n",
    "model.add(Dense(128, activation='relu'))  # Hidden layer\n",
    "# TODO: Add output layer.\n",
    "model.compile(sgd(lr=.01), \"mse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "This part is the same as with Q-learning.\n",
    "\n",
    "**Task:** Pick exploration probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimal_action(state):\n",
    "    q = model.predict(convert_state_to_input(state))\n",
    "    return Action.get_all_actions()[np.argmax(q[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(game_state):\n",
    "    exploration_prob = 0.20\n",
    "    if exploration_prob > np.random.rand():\n",
    "        # Explore\n",
    "        return np.random.choice(Action.get_all_actions())\n",
    "    else:\n",
    "        # Exploit\n",
    "        return pick_optimal_action(game_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "The experience replay implementation is just procedural and is fully implemented below. The memory is implemented as a deque with a max size. Thus, when the memory is full, new experiences will push out old experiences as in a queue.\n",
    "\n",
    "The memory is filled with Experience objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def get(self, index):\n",
    "        return self.memory[index]\n",
    "\n",
    "    def get_mini_batch(self, batch_size):\n",
    "        memory_size = self.get_size()\n",
    "        indices = np.random.choice(np.arange(memory_size), min(batch_size, memory_size), replace=False)\n",
    "        return [self.memory[i] for i in indices]\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Experience:\n",
    "\n",
    "    def __init__(self, current_state, action, reward, next_state, done: bool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            done (bool):\n",
    "            current_state (GameState):\n",
    "            action (Action):\n",
    "            reward (int):\n",
    "            next_state (GameState):\n",
    "        \"\"\"\n",
    "        self.current_state = current_state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Most of the intricate details are already implemented.\n",
    "\n",
    "**Tasks:** \n",
    "1. Assign a new Experience object to the experience variable. Use convert_state_to_input to convert the game states before storing them in the Experience.\n",
    "\n",
    "\n",
    "2. Add the experience to the memory.\n",
    "\n",
    "\n",
    "3. Get a mini batch to train on and assign to batch\n",
    "\n",
    "\n",
    "4. Run model.train_on_batch() and pass x_train and np.asarray(y_train) as arguments. Assign this to batch_loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(level, num_training_episodes, batch_size, gamma=0.9):\n",
    "    initial_game_state = initialize_gamestate_from_file(level)\n",
    "    tot_loss = {}\n",
    "    memory = Memory(max_size=5000)\n",
    "\n",
    "    for i in range(1, num_training_episodes):\n",
    "\n",
    "        loss = 0.\n",
    "        num_episode_steps = 0\n",
    "\n",
    "        done = False\n",
    "        current_game_state = deepcopy(initial_game_state)\n",
    "\n",
    "        while not done:\n",
    "            if num_episode_steps > 500:\n",
    "                break\n",
    "\n",
    "            action = pick_action(current_game_state)\n",
    "            next_game_state, action_event = get_next_game_state_from_action(current_game_state, action.name)\n",
    "\n",
    "            if action_event == ActionEvent.WON or action_event == ActionEvent.LOST:\n",
    "                done = True\n",
    "                if action_event == ActionEvent.WON:\n",
    "                    print(\"Won!!\")\n",
    "                else:\n",
    "                    print('lost')\n",
    "\n",
    "            reward = calculate_reward_for_move(action_event)\n",
    "            \n",
    "            # TODO: Store new experience\n",
    "            experience = \n",
    "            # TODO: Add the experience to the memory.\n",
    "    \n",
    "            # TODO: Get a mini batch to train on from memory.\n",
    "            batch = \n",
    "\n",
    "            # Dimensions of our observed states, ie, the input to our model.\n",
    "            input_dim = batch[0].current_state.shape[1]\n",
    "            x_train = np.zeros((min(memory.get_size(), batch_size), input_dim))\n",
    "            y_train = np.zeros((x_train.shape[0], len(Action.get_all_actions())))  # Target Q-value\n",
    "\n",
    "            sample: Experience\n",
    "            for j, sample in enumerate(batch):\n",
    "                y_target = model.predict(sample.current_state)[0]\n",
    "\n",
    "                x_train[j:j + 1] = sample.current_state\n",
    "                if sample.done:\n",
    "                    y_target[sample.action.value] = sample.reward\n",
    "                else:\n",
    "                    y_target[sample.action.value] = sample.reward + gamma * np.max(model.predict(sample.next_state))\n",
    "                y_train[j] = y_target\n",
    "\n",
    "            batch_loss = model.train_on_batch(x_train, np.asarray(y_train))\n",
    "\n",
    "            loss += batch_loss\n",
    "\n",
    "            num_episode_steps += 1\n",
    "\n",
    "            current_game_state = deepcopy(next_game_state)\n",
    "\n",
    "        print(i)\n",
    "        print(loss / num_episode_steps)\n",
    "\n",
    "        tot_loss[i] = (loss / num_episode_steps)\n",
    "\n",
    "    print(tot_loss)\n",
    "\n",
    "    # plot_training_history(tot_loss)\n",
    "\n",
    "    model.save('./nn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:** \n",
    "\n",
    "1. Train the model, download the file nn_model.h5. Store it locally in the jupyter folder, and execute play_deep_q_model() in jupyter_main. \n",
    "\n",
    "\n",
    "2. Try to improve the model by testing different parameters, such as the neural network architecture, number of hidden neurons, number of hidden layers etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(level=level, num_training_episodes=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
