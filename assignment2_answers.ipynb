{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "nbimporter.options['only_defs'] = False\n",
    "from pacman_game import Action, initialize_gamestate_from_file, get_next_game_state_from_action, ActionEvent\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.07\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_for_move(action_event):\n",
    "    if action_event == ActionEvent.DOT:\n",
    "        return 1\n",
    "    elif action_event == ActionEvent.CAPTURED_BY_GHOST:\n",
    "        return -5\n",
    "    elif action_event == ActionEvent.NONE:\n",
    "        return -0.1\n",
    "    elif action_event == ActionEvent.WALL:\n",
    "        return -0.1\n",
    "    elif action_event == ActionEvent.WON:\n",
    "        return 10\n",
    "    elif action_event == ActionEvent.LOST:\n",
    "        return -10\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_input(state):\n",
    "    string_rep = state.__str__()\n",
    "    r = np.array([])\n",
    "\n",
    "    for char in string_rep:\n",
    "        if char == 'o':\n",
    "            r = np.concatenate([r, [0, 0, 0, 0, 1]])\n",
    "        if char == ' ':\n",
    "            r = np.concatenate([r, [0, 0, 0, 1, 0]])\n",
    "        if char == 'P':\n",
    "            r = np.concatenate([r, [0, 0, 1, 0, 0]])\n",
    "        if char == 'G':\n",
    "            r = np.concatenate([r, [0, 1, 0, 0, 0]])\n",
    "        if char == '.':\n",
    "            r = np.concatenate([r, [1, 0, 0, 0, 0]])\n",
    "\n",
    "    return r.reshape(1, r.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE MODEL ##\n",
    "level = 'level-0'\n",
    "initial_game_state = initialize_gamestate_from_file(level)\n",
    "\n",
    "input_size = convert_state_to_input(initial_game_state).size\n",
    "num_actions = len(Action.get_all_actions())\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(SGD(lr=.01), \"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimal_action(state):\n",
    "    q = model.predict(convert_state_to_input(state))\n",
    "    return Action.get_all_actions()[np.argmax(q[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(game_state):\n",
    "    exploration_prob = 0.20\n",
    "    if exploration_prob > np.random.rand():\n",
    "        # Explore\n",
    "        return np.random.choice(Action.get_all_actions())\n",
    "    else:\n",
    "        # Exploit\n",
    "        return pick_optimal_action(game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def get(self, index):\n",
    "        return self.memory[index]\n",
    "\n",
    "    def get_mini_batch(self, batch_size):\n",
    "        memory_size = self.get_size()\n",
    "        indices = np.random.choice(np.arange(memory_size), min(batch_size, memory_size), replace=False)\n",
    "        return [self.memory[i] for i in indices]\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Experience:\n",
    "\n",
    "    def __init__(self, current_state, action, reward, next_state, done: bool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            done (bool):\n",
    "            current_state (GameState):\n",
    "            action (Action):\n",
    "            reward (int):\n",
    "            next_state (GameState):\n",
    "        \"\"\"\n",
    "        self.current_state = current_state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(level, num_training_episodes, batch_size, gamma=0.9):\n",
    "    print(\"Start training\")\n",
    "    \n",
    "    initial_game_state = initialize_gamestate_from_file(level)\n",
    "    tot_loss = {}\n",
    "    memory = Memory(max_size=5000)\n",
    "\n",
    "    for i in range(1, num_training_episodes):\n",
    "        print(\"\\nEpisode number\", i)\n",
    "        \n",
    "        loss = 0.\n",
    "        num_episode_steps = 0\n",
    "\n",
    "        done = False\n",
    "        current_game_state = deepcopy(initial_game_state)\n",
    "\n",
    "        while not done:\n",
    "            if num_episode_steps > 500:\n",
    "                break\n",
    "\n",
    "            action = pick_action(current_game_state)\n",
    "            next_game_state, action_event = get_next_game_state_from_action(current_game_state, action.name)\n",
    "\n",
    "            if action_event == ActionEvent.WON or action_event == ActionEvent.LOST:\n",
    "                done = True\n",
    "                if action_event == ActionEvent.WON:\n",
    "                    print(\"Won!!\")\n",
    "                else:\n",
    "                    print(\"Lost\")\n",
    "\n",
    "            reward = calculate_reward_for_move(action_event)\n",
    "\n",
    "            experience = Experience(\n",
    "                current_state=convert_state_to_input(current_game_state),\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                next_state=convert_state_to_input(next_game_state),\n",
    "                done=done\n",
    "            )\n",
    "            memory.add(experience)\n",
    "\n",
    "            batch = memory.get_mini_batch(batch_size=batch_size)\n",
    "\n",
    "            # Dimensions of our observed states, ie, the input to our model.\n",
    "            input_dim = batch[0].current_state.shape[1]\n",
    "            x_train = np.zeros((min(memory.get_size(), batch_size), input_dim))\n",
    "            y_train = np.zeros((x_train.shape[0], len(Action.get_all_actions())))  # Target Q-value\n",
    "\n",
    "            sample: Experience\n",
    "            for j, sample in enumerate(batch):\n",
    "                y_target = model.predict(sample.current_state)[0]\n",
    "\n",
    "                x_train[j:j + 1] = sample.current_state\n",
    "                if sample.done:\n",
    "                    y_target[sample.action.value] = sample.reward\n",
    "                else:\n",
    "                    y_target[sample.action.value] = sample.reward + gamma * np.max(model.predict(sample.next_state))\n",
    "                y_train[j] = y_target\n",
    "\n",
    "            batch_loss = model.train_on_batch(x_train, np.asarray(y_train))\n",
    "\n",
    "            loss += batch_loss\n",
    "\n",
    "            num_episode_steps += 1\n",
    "\n",
    "            current_game_state = deepcopy(next_game_state)\n",
    "\n",
    "        print(\"Number of moves:\", num_episode_steps)\n",
    "        print(\"Loss:\", loss)\n",
    "        print(\"Loss per step/move:\", loss / num_episode_steps)\n",
    "\n",
    "        tot_loss[i] = (loss / num_episode_steps)\n",
    "\n",
    "    print(\"\\nFinished training\")\n",
    "    print(\"\\nTotal loss in each episode\\n\", tot_loss)\n",
    "\n",
    "    # plot_training_history(tot_loss)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    model_path = \"nn_model.h5\"\n",
    "    model.save('./' + model_path)\n",
    "    \n",
    "    print(\"Model saved to \" + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(level=level, num_training_episodes=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
