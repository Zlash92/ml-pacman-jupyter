{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Q-Learning\n",
    "\n",
    "Hello there! In this task you will implement a Q-learning which can play Pac-Man. \n",
    "\n",
    "Jupyter notebook consists of code cells. To run code, select a cell and press Shift + Enter. This will execute the particular code. The space below the code will work as a traditional console, with prints. Do the assignment by working you way through tasks and code cells. If you change something in a code cell, it must be ran again for the change to propagate.\n",
    "\n",
    "Some cells are fully implemented already, and the behaviour of the code is described, while in other cells you have to do some coding yourself. The tasks are designed in such a way that you should not have to implement the most intricate details of the algorithm, which can be time-consuming. It is more useful to spend more time experimenting with parameters ;)\n",
    "\n",
    "**Task**: Run the code cell below to import necessary packages and methods. Simple enough ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "nbimporter.options['only_defs'] = False\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from file_utils import save_pickle, load_pickle\n",
    "from pacman_game import Action, ActionEvent, get_next_game_state_from_action, initialize_gamestate_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "We need to define reward values for different situations which can occur in the Pac-Man environment.\n",
    "\n",
    "**Task**: Define reward values for each event that can occur when Pac-Man performs an action. In some cases, the algorithm can be very sensitive with regards to the reward values, but try values in the range [0.1, 20]. Remember: Negative rewards for negative events and positive reward for positive events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_for_move(action_event):\n",
    "    if action_event == ActionEvent.DOT:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.CAPTURED_BY_GHOST:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.NONE:\n",
    "        return -0.1\n",
    "    elif action_event == ActionEvent.WALL:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.WON:\n",
    "        # TODO\n",
    "    elif action_event == ActionEvent.LOST:\n",
    "        # TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table is a dictionary of dictionaries. More precisely, it is a dictionary where the key is game state, and the value is another dictionary mapping to the possible actions in the game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "For a particular state, we find the most optimal action by looking up the state in the Q-table, and finding the action with highest Q-value. If there is tie, which will especially happen initially, a random action is picked from these actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimal_action(state, printing=False):\n",
    "    max_value = max(q_table[state].values())\n",
    "    actions = [key for key in q_table[state] if q_table[state][key] == max_value]\n",
    "\n",
    "    if printing:\n",
    "        print(state)\n",
    "        print(q_table[state])\n",
    "\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "Implement pick action, which should return an Action.\n",
    "1. Choose the exploration probability.\n",
    "\n",
    "\n",
    "2. Implement the exploration policy: Pick a random action. Use Action.get_all_actions() to get a list of all possible actions. To pick a random action, use np.random.choice(), which returns a random element from the list passed as argument.\n",
    "\n",
    "\n",
    "3. Implement the exploit policy. Remember: We have already implemented the method for picking the optimal action ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(game_state):\n",
    "    # TODO: Pick exploration_prob. Range: [0.0, 1.0]\n",
    "    exploration_prob = None\n",
    "    \n",
    "    # Initialize game state with zeros as Q-values if the game state is not present.\n",
    "    if game_state not in q_table:\n",
    "        q_table[game_state] = {key: 0.0 for key in Action.get_all_actions()}\n",
    "        \n",
    "    if exploration_prob > np.random.rand():\n",
    "        # TODO: Explore\n",
    "    else:\n",
    "        # TODO: Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_q_value(state):\n",
    "    \"\"\"\n",
    "    Computes Q-value for a state\n",
    "    \"\"\"\n",
    "    if state not in q_table:\n",
    "        q_table[state] = {key: 0.0 for key in Action.get_all_actions()}\n",
    "\n",
    "    return max(q_table[state].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**Tasks:**\n",
    "1. Pick a value for discount. A sensible range is [0.8, 1.0]\n",
    "\n",
    "\n",
    "2. Pick a value for alpha. A sensible range is [0.001, 0.3]\n",
    "\n",
    "\n",
    "3. Assign a value for which action to perform, using implemented methods.\n",
    "\n",
    "\n",
    "4. Assign a value for reward, using implemented methods.\n",
    "\n",
    "\n",
    "5. Implement the Q-value update by accessing the correct state in the Q-table, and the correct action in the state dictionary. Use compute_max_q_value to compute the maximum Q-value for the next game state.\n",
    "\n",
    "![title](q_learning_update_eq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(level='level-0', num_episodes=10):\n",
    "    initial_game_state = initialize_gamestate_from_file(level)\n",
    "    \n",
    "    # TODO: Choose discount and alpha values\n",
    "    discount = \n",
    "    alpha = \n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        current_game_state = deepcopy(initial_game_state)\n",
    "        episode_step = 0\n",
    "        episode_done = False\n",
    "        if i % 50 == 0:\n",
    "                print(\"Episode number\", i)\n",
    "        while not episode_done:\n",
    "            \n",
    "            # TODO: Get action\n",
    "            action = \n",
    "            new_game_state, action_event = get_next_game_state_from_action(current_game_state, action.name)\n",
    "\n",
    "            if action_event == ActionEvent.WON or action_event == ActionEvent.LOST:\n",
    "                episode_done = True\n",
    "                if action_event == ActionEvent.WON:\n",
    "                    print(\"Won!!\")\n",
    "\n",
    "            reward = \n",
    "\n",
    "            if current_game_state not in q_table:\n",
    "                q_table[current_game_state] = {key: 0.0 for key in Action.get_all_actions()}\n",
    "        \n",
    "            # TODO: Implement Q-Value update\n",
    "            q_table[current_game_state][action] = \n",
    "\n",
    "            current_game_state = new_game_state\n",
    "            \n",
    "            episode_step += 1\n",
    "            if episode_step > 500:\n",
    "                break\n",
    "\n",
    "    print(\"\\nFinished training\")\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    file_name = \"q_table\"\n",
    "    save_pickle('./' + file_name, q_table, True)\n",
    "\n",
    "    print(\"Model saved to \" + file_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Train the model. num_episodes defines how many Pac-Man games to train for.\n",
    "\n",
    "Unfortunately, we can't watch our AI play Pac-Man in jupyter. So, after training:\n",
    " 1. Download the file containing our model (q_table.pkl) and place it in the jupyter folder in the project ml-pacman.\n",
    " 2. In jupyter_main.py, uncomment the call to play_q_learning_model() at the bottom and comment out play_deep_q_model() and test_setup().\n",
    " 3. Run jupter_main to see the agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_episodes=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
